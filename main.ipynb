{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('data-ga-en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ga          english\n",
      "0  Bo ni otswa mi aloo?  Did you call me\n",
      "1  Bo ni otswa mi aloo?  Did you call me\n",
      "2  Bo ni otswa mi aloo?  Did you call me\n",
      "3  Bo ni otswa mi aloo?  Did you call me\n",
      "4  Bo ni otswa mi aloo?  Did you call me\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "\n",
    "# Separate the columns into lists\n",
    "twi_texts = data['ga'].tolist()\n",
    "english_texts = data['english'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start> \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m <end>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m texts\n\u001b[1;32m----> 7\u001b[0m twi_texts \u001b[38;5;241m=\u001b[39m preprocess_texts(twi_texts)\n\u001b[0;32m      8\u001b[0m english_texts \u001b[38;5;241m=\u001b[39m preprocess_texts(english_texts)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Tokenize the texts\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mpreprocess_texts\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_texts\u001b[39m(texts):\n\u001b[1;32m----> 3\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m      4\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start> \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m <end>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m texts\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_texts\u001b[39m(texts):\n\u001b[1;32m----> 3\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m      4\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start> \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m <end>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m texts\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess the texts\n",
    "def preprocess_texts(texts):\n",
    "    texts = [text.lower() for text in texts]\n",
    "    texts = ['<start> ' + text + ' <end>' for text in texts]\n",
    "    return texts\n",
    "\n",
    "twi_texts = preprocess_texts(twi_texts)\n",
    "english_texts = preprocess_texts(english_texts)\n",
    "\n",
    "# Tokenize the texts\n",
    "twi_tokenizer = Tokenizer()\n",
    "twi_tokenizer.fit_on_texts(twi_texts)\n",
    "twi_sequences = twi_tokenizer.texts_to_sequences(twi_texts)\n",
    "\n",
    "english_tokenizer = Tokenizer()\n",
    "english_tokenizer.fit_on_texts(english_texts)\n",
    "english_sequences = english_tokenizer.texts_to_sequences(english_texts)\n",
    "\n",
    "# Pad the sequences\n",
    "max_twi_len = max([len(seq) for seq in twi_sequences])\n",
    "max_english_len = max([len(seq) for seq in english_sequences])\n",
    "\n",
    "twi_sequences = pad_sequences(twi_sequences, maxlen=max_twi_len, padding='post')\n",
    "english_sequences = pad_sequences(english_sequences, maxlen=max_english_len, padding='post')\n",
    "\n",
    "# Prepare the input and output sequences\n",
    "encoder_input_data = np.array(twi_sequences)\n",
    "decoder_input_data = np.array(english_sequences[:, :-1])\n",
    "decoder_output_data = np.array(english_sequences[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twi_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, LSTM, Embedding, Dense\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the input dimensions\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m twi_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(twi_tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m english_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(english_tokenizer\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      7\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twi_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "# Define the input dimensions\n",
    "twi_vocab_size = len(twi_tokenizer.word_index) + 1\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "embedding_dim = 256\n",
    "latent_dim = 512\n",
    "\n",
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(input_dim=twi_vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=english_vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(english_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mfit([encoder_input_data, decoder_input_data], \n\u001b[0;32m      5\u001b[0m           decoder_output_data, \n\u001b[0;32m      6\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[0;32m      7\u001b[0m           epochs\u001b[38;5;241m=\u001b[39mepochs, \n\u001b[0;32m      8\u001b[0m           validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_output_data, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Twi text file\n",
    "twi_data = pd.read_csv('twi.csv', header=None, names=['Twi'], delimiter=',', error_bad_lines=False)\n",
    "# Fill missing values with an empty string\n",
    "twi_data['Twi'] = twi_data['Twi'].fillna('')\n",
    "\n",
    "# Load the English text file\n",
    "english_data = pd.read_csv('english.csv', header=None, names=['English'], delimiter=',', error_bad_lines=False)\n",
    "# Fill missing values with an empty string\n",
    "english_data['English'] = english_data['English'].fillna('')\n",
    "\n",
    "# Convert the data to lists\n",
    "twi_texts = twi_data['Twi'].tolist()\n",
    "english_texts = english_data['English'].tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
