{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3098e25-70eb-4799-afa0-b21fb26b6df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   twi                 english Unnamed: 2\n",
      "0  Maa wɔn ho yɛɛ huam  For them to smell good        NaN\n",
      "1  Maa wɔn ho yɛɛ huam  For them to smell good        NaN\n",
      "2  Maa wɔn ho yɛɛ huam  For them to smell good        NaN\n",
      "3  Maa wɔn ho yɛɛ huam  For them to smell good        NaN\n",
      "4  Maa wɔn ho yɛɛ huam  For them to smell good        NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data-tw-en.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d14289-aa52-429e-bbf7-12c0ec7b4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 twi                               english  \\\n",
      "0  <start> maa wɔn ho yɛɛ huam <end>  <start> for them to smell good <end>   \n",
      "1  <start> maa wɔn ho yɛɛ huam <end>  <start> for them to smell good <end>   \n",
      "2  <start> maa wɔn ho yɛɛ huam <end>  <start> for them to smell good <end>   \n",
      "3  <start> maa wɔn ho yɛɛ huam <end>  <start> for them to smell good <end>   \n",
      "4  <start> maa wɔn ho yɛɛ huam <end>  <start> for them to smell good <end>   \n",
      "\n",
      "  Unnamed: 2  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_texts(texts):\n",
    "    texts = texts.fillna('')  # Fill missing values\n",
    "    texts = texts.str.lower()  # Convert to lower case\n",
    "    texts = '<start> ' + texts + ' <end>'  # Add start and end tokens\n",
    "    return texts\n",
    "\n",
    "# Preprocess the texts\n",
    "data['twi'] = preprocess_texts(data['twi'])\n",
    "data['english'] = preprocess_texts(data['english'])\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34a2dca-56dd-49b8-b182-563e690c957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create a tokenizer for Twi texts\n",
    "twi_tokenizer = Tokenizer()\n",
    "twi_tokenizer.fit_on_texts(data['twi'])\n",
    "twi_sequences = twi_tokenizer.texts_to_sequences(data['twi'])\n",
    "twi_vocab_size = len(twi_tokenizer.word_index) + 1\n",
    "\n",
    "# Create a tokenizer for English texts\n",
    "english_tokenizer = Tokenizer()\n",
    "english_tokenizer.fit_on_texts(data['english'])\n",
    "english_sequences = english_tokenizer.texts_to_sequences(data['english'])\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "\n",
    "# Pad the sequences\n",
    "max_len = max(max(len(seq) for seq in twi_sequences), max(len(seq) for seq in english_sequences))\n",
    "ga_sequences = pad_sequences(twi_sequences, maxlen=max_len, padding='post')\n",
    "english_sequences = pad_sequences(english_sequences, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac7b190-d512-4103-b869-48745d74a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Prepare encoder input data\n",
    "encoder_input_data = np.array(ga_sequences)\n",
    "\n",
    "# Prepare decoder input and output data\n",
    "decoder_input_data = np.array(english_sequences)\n",
    "decoder_output_data = np.zeros_like(decoder_input_data)\n",
    "\n",
    "# Shift the decoder output data by one timestep\n",
    "decoder_output_data[:, :-1] = decoder_input_data[:, 1:]\n",
    "decoder_output_data[:, -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "926a97e8-2133-4f72-872f-d08f75224fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 441ms/step - loss: 1.3716 - val_loss: 1.8981\n",
      "Epoch 2/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 399ms/step - loss: 0.6575 - val_loss: 2.2914\n",
      "Epoch 3/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 367ms/step - loss: 0.2797 - val_loss: 2.7258\n",
      "Epoch 4/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m974s\u001b[0m 3s/step - loss: 0.1488 - val_loss: 2.9601\n",
      "Epoch 5/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 372ms/step - loss: 0.0675 - val_loss: 3.1690\n",
      "Epoch 6/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24777s\u001b[0m 71s/step - loss: 0.0265 - val_loss: 3.3590\n",
      "Epoch 7/100\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m994s\u001b[0m 3s/step - loss: 0.0125 - val_loss: 3.4750\n",
      "Epoch 8/100\n",
      "\u001b[1m307/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 362ms/step - loss: 0.0141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = 256\n",
    "latent_dim = 512\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(twi_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(english_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(english_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data],\n",
    "          np.expand_dims(decoder_output_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f8b3f-d14e-4a60-9e62-d25c37d94337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
